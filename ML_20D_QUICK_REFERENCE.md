# Quick Reference â€” ML 20D Deployment

## ğŸš€ Deploy in 3 Steps

```bash
# 1. Verify files exist
ls models/model_20d_v2.pkl experiments/training_dataset_20d_v2.csv

# 2. Test locally
streamlit run stock_scout.py

# 3. Enable toggles â†’ Check ENABLE_ML & USE_FINAL_SCORE_SORT
```

---

## ğŸ“Š Key Metrics at a Glance

| Metric | Result |
|--------|--------|
| **Model ROC-AUC** | 0.777 |
| **Top 10% Bucket Return** | +0.0120 vs -0.0164 baseline |
| **Top 10% Hit Rate** | 24.8% vs 8.7% baseline |
| **Dataset Size** | 20,547 rows Ã— 62 tickers |
| **Training Window** | 2.5 years (Jan 2023 â€“ Mar 2025) |

---

## ğŸ¯ What Changed

### In `stock_scout.py`
- âœ… Preflight check on first run (API health)
- âœ… `ENABLE_ML` toggle (show/hide ML_20d_Prob)
- âœ… `USE_FINAL_SCORE_SORT` toggle (sort by FinalScore)
- âœ… API status line ("APIs: X OK / Y down")
- âœ… FinalScore rank-based (0.5 tech + 0.5 ML)

### In `core/data_sources_v2.py`
- âœ… `provider_status` parameter passed to all API calls
- âœ… Preflight checks skip disabled providers automatically

### In `core/ml_20d_inference.py`
- âœ… Model v2 loading (with v1 fallback)

### New Files
- âœ… `models/model_20d_v2.pkl` (trained model)
- âœ… `experiments/training_dataset_20d_v2.csv` (dataset)
- âœ… `experiments/train_ml_20d_v2.py` (training script)

---

## ğŸ”§ Common Tasks

### View ML Rankings Offline
```bash
python -m experiments.offline_recommendation_audit \
  --mode audit_ml_20d \
  --input experiments/training_dataset_20d_v2.csv \
  --output audit.csv
```

### Retrain Model (Quarterly)
```bash
# Update date range in audit script, then:
python -m experiments.train_ml_20d_v2 \
  experiments/training_dataset_20d_v2.csv
```

### Validate Improvements
```bash
python experiments/validate_ml_improvements.py
```

### Check Model Health
```python
from core.ml_20d_inference import ML_20D_AVAILABLE, predict_20d_prob_from_row
print(f"Model available: {ML_20D_AVAILABLE}")
prob = predict_20d_prob_from_row(sample_row)  # Returns [0, 1]
```

---

## ğŸ“ˆ FinalScore Formula

```
FinalScore = (0.5 Ã— percentile_rank(TechScore) + 0.5 Ã— percentile_rank(ML_Prob)) Ã— 100
```

**Weights:** 0.5/0.5 (tech/ML)  
**Range:** 0â€“100  
**Sorting:** Higher FinalScore = higher predicted probability of 15%+ 20d return

---

## âš ï¸ Known Limitations

1. **TechScore weak signal** (correlation +0.0105 with forward returns)
2. **Class imbalance** (8.8% positive labels)
3. **Dataset stale after Mar 26, 2025** (needs quarterly refresh)
4. **FinalScore deciles show soft ranking** (consider 0.3/0.7 weight for stronger signal)

---

## âœ… Production Checklist

Before deploying:
- [ ] `models/model_20d_v2.pkl` exists (101.5 KB)
- [ ] `experiments/training_dataset_20d_v2.csv` exists (4.0 MB)
- [ ] `python -m py_compile stock_scout.py` runs without error
- [ ] `streamlit run stock_scout.py` shows ML toggle in sidebar
- [ ] Toggle `ENABLE_ML` displays ML_20d_Prob in cards
- [ ] Toggle `USE_FINAL_SCORE_SORT` sorts by FinalScore

---

## ğŸ“š Documentation

- [ML_20D_DELIVERY_SUMMARY.md](ML_20D_DELIVERY_SUMMARY.md) â€” Full technical details
- [ML_20D_INTEGRATION_INDEX.md](ML_20D_INTEGRATION_INDEX.md) â€” Architecture & usage
- [experiments/validate_ml_improvements.py](experiments/validate_ml_improvements.py) â€” Validation script

---

## ğŸ¯ Success Criteria (Met)

âœ… Top 10% by ML probability outperforms baseline by +2.84% absolute return  
âœ… ML_20d_Prob display unified and consistent across cards  
âœ… FinalScore computed via rank-based formula (0.5/0.5 weighting)  
âœ… Preflight integration reduces API errors  
âœ… Backward compatibility maintained  
âœ… All code compiles without errors  

---

**Status: âœ… READY FOR PRODUCTION**

Last Updated: December 25, 2025
