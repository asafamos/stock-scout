# -*- coding: utf-8 -*-
"""
Offline Signal Analyzer
-----------------------
Reads one or more offline snapshot / big-winners CSV files and:
- Analyzes how technical features relate to future returns
- Finds which signals are actually predictive
- Exports correlation tables and bin statistics for manual review

Usage examples:

  # Basic: single snapshot file
  python -m experiments.offline_signal_analyzer \
    --snapshot-files experiments/snapshot_2024_03_15_offline.csv \
    --output-prefix experiments/analysis_2024_03_15

  # Snapshot + big winners file
  python -m experiments.offline_signal_analyzer \
    --snapshot-files experiments/snapshot_2024_03_15_offline.csv \
    --big-winners-files experiments/big_winners_2024_offline.csv \
    --output-prefix experiments/analysis_2024_full

Notes:
- This script is completely offline: NO API calls, NO yfinance.
- It only analyzes CSVs that were already generated by offline_recommendation_audit.py
"""

import argparse
import os
from typing import List, Optional, Dict

import numpy as np
import pandas as pd


# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------

def load_csv_list(paths_arg: Optional[str], label: str) -> pd.DataFrame:
    """
    Load and concatenate a comma-separated list of CSV paths.
    Returns an empty DataFrame if paths_arg is None/empty.
    """
    if not paths_arg:
        print(f"[INFO] No {label} files provided.")
        return pd.DataFrame()

    paths = [p.strip() for p in paths_arg.split(",") if p.strip()]
    frames: List[pd.DataFrame] = []

    for p in paths:
        if not os.path.exists(p):
            print(f"[WARN] {label} file not found: {p}")
            continue
        try:
            df = pd.read_csv(p)
            df["__source_file"] = os.path.basename(p)
            frames.append(df)
            print(f"[INFO] Loaded {label} file: {p} | rows={len(df)}")
        except Exception as e:
            print(f"[ERROR] Failed to read {label} file {p}: {e}")

    if not frames:
        print(f"[WARN] No valid {label} data loaded.")
        return pd.DataFrame()

    combined = pd.concat(frames, ignore_index=True)
    print(f"[INFO] Combined {label} rows: {len(combined)} from {len(frames)} file(s)")
    return combined


def detect_return_columns(df: pd.DataFrame) -> List[str]:
    """
    Detect columns that look like forward return columns (e.g. Return_5d, Return_20d).
    """
    if df.empty:
        return []

    candidates = [c for c in df.columns if c.lower().startswith("return_")]
    # Filter to numeric
    return_cols = []
    for c in candidates:
        if pd.api.types.is_numeric_dtype(df[c]):
            return_cols.append(c)

    return return_cols


def compute_correlations(
    df: pd.DataFrame,
    features: List[str],
    return_cols: List[str]
) -> pd.DataFrame:
    """
    Compute Pearson correlations between each feature and each return horizon.
    Returns a wide table: index = feature, columns = return horizons.
    """
    records = []
    for feat in features:
        if feat not in df.columns:
            continue
        for ret_col in return_cols:
            sub = df[[feat, ret_col]].dropna()
            if len(sub) < 30:
                corr = np.nan
                n = len(sub)
            else:
                corr = sub[feat].corr(sub[ret_col])
                n = len(sub)
            records.append({
                "Feature": feat,
                "Return_Column": ret_col,
                "Correlation": corr,
                "N": n,
            })

    corr_df = pd.DataFrame(records)
    if corr_df.empty:
        return corr_df

    # Pivot to wide format
    wide = corr_df.pivot(index="Feature", columns="Return_Column", values="Correlation")
    wide["N_min"] = corr_df.groupby("Feature")["N"].min()
    wide["N_max"] = corr_df.groupby("Feature")["N"].max()
    return wide.reset_index()


def compute_feature_bins(
    df: pd.DataFrame,
    feature: str,
    target: str,
    q: int = 5
) -> Optional[pd.DataFrame]:
    """
    Bin a single feature into quantiles and compute mean target per bin.

    Returns a DataFrame with:
      - Feature
      - Bin (interval)
      - Count
      - Mean_Target
    or None if not enough data.
    """
    if feature not in df.columns or target not in df.columns:
        return None

    sub = df[[feature, target]].dropna().copy()
    if len(sub) < q:
        return None

    try:
        sub["bin"] = pd.qcut(sub[feature], q=q, duplicates="drop")
    except Exception as e:
        print(f"[WARN] Could not bin feature {feature}: {e}")
        return None

    grouped = sub.groupby("bin")[target].agg(["count", "mean"]).reset_index()
    grouped.insert(0, "Feature", feature)
    grouped.rename(columns={"mean": f"Mean_{target}", "count": "Count"}, inplace=True)
    return grouped


def summarize_big_winners(df_bw: pd.DataFrame) -> Dict[str, float]:
    """
    Simple summary stats for big_winners file.
    Assumes columns:
      - Forward_Return
      - TechScore_20d
    """
    if df_bw.empty:
        return {}

    stats = {}
    if "Forward_Return" in df_bw.columns:
        stats["Forward_Return_mean"] = float(df_bw["Forward_Return"].mean())
        stats["Forward_Return_median"] = float(df_bw["Forward_Return"].median())
        stats["Forward_Return_min"] = float(df_bw["Forward_Return"].min())
        stats["Forward_Return_max"] = float(df_bw["Forward_Return"].max())
        stats["Forward_Return_N"] = int(df_bw["Forward_Return"].notna().sum())

    if "TechScore_20d" in df_bw.columns:
        stats["TechScore_20d_mean"] = float(df_bw["TechScore_20d"].mean())
        stats["TechScore_20d_median"] = float(df_bw["TechScore_20d"].median())
        stats["TechScore_20d_min"] = float(df_bw["TechScore_20d"].min())
        stats["TechScore_20d_max"] = float(df_bw["TechScore_20d"].max())

    return stats


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="Offline technical signal analyzer")
    parser.add_argument(
        "--snapshot-files",
        type=str,
        help="Comma-separated list of snapshot CSV files (from offline_recommendation_audit.py --mode snapshot)"
    )
    parser.add_argument(
        "--big-winners-files",
        type=str,
        default=None,
        help="Comma-separated list of big winners CSV files (from --mode big_winners) [optional]"
    )
    parser.add_argument(
        "--output-prefix",
        type=str,
        default="experiments/offline_analysis",
        help="Prefix for output CSVs (default: experiments/offline_analysis)"
    )
    parser.add_argument(
        "--bins",
        type=int,
        default=5,
        help="Number of quantile bins for feature analysis (default: 5)"
    )

    args = parser.parse_args()

    # 1) Load data
    snapshot_df = load_csv_list(args.snapshot_files, label="snapshot")
    bigw_df = load_csv_list(args.big_winners_files, label="big_winners") if args.big_winners_files else pd.DataFrame()

    if snapshot_df.empty:
        print("[ERROR] No snapshot data loaded. Nothing to analyze.")
        return

    # 2) Detect return columns & features
    return_cols = detect_return_columns(snapshot_df)
    if not return_cols:
        print("[ERROR] No forward return columns detected (e.g. Return_5d, Return_20d).")
        return

    # candidate features based on your offline schema
    candidate_features = [
        "TechScore_20d",
        "RSI",
        "ATR_Pct",
        "RR",
        "MomCons",
        "VolSurge",
    ]
    features = [f for f in candidate_features if f in snapshot_df.columns]

    print(f"[INFO] Detected return columns: {return_cols}")
    print(f"[INFO] Using features: {features}")

    # 3) Correlations
    corr_df = compute_correlations(snapshot_df, features, return_cols)
    corr_path = f"{args.output_prefix}_correlations.csv"
    corr_df.to_csv(corr_path, index=False)
    print(f"[DONE] Saved correlations to {corr_path}")

    # 4) Feature bins per horizon
    all_bins_frames: List[pd.DataFrame] = []
    for ret_col in return_cols:
        for feat in features:
            bins_df = compute_feature_bins(snapshot_df, feat, ret_col, q=args.bins)
            if bins_df is None or bins_df.empty:
                continue
            bins_df.insert(1, "Target", ret_col)
            all_bins_frames.append(bins_df)

    if all_bins_frames:
        bins_all = pd.concat(all_bins_frames, ignore_index=True)
        bins_path = f"{args.output_prefix}_feature_bins.csv"
        bins_all.to_csv(bins_path, index=False)
        print(f"[DONE] Saved feature-bin stats to {bins_path}")
    else:
        print("[WARN] No feature-bin statistics produced (not enough data).")

    # 5) Big winners summary (optional)
    if not bigw_df.empty:
        stats = summarize_big_winners(bigw_df)
        stats_rows = [{"Metric": k, "Value": v} for k, v in stats.items()]
        stats_df = pd.DataFrame(stats_rows)
        stats_path = f"{args.output_prefix}_big_winners_summary.csv"
        stats_df.to_csv(stats_path, index=False)
        print(f"[DONE] Saved big-winners summary to {stats_path}")
    else:
        print("[INFO] No big_winners data provided; skipping big-winners summary.")

    # 6) Console summary (quick human view)
    print("\n=== QUICK SUMMARY ===")
    if not corr_df.empty:
        # flatten correlations for console
        flat_corr = []
        for _, row in corr_df.iterrows():
            feat = row["Feature"]
            for ret_col in return_cols:
                if ret_col in row and pd.notna(row[ret_col]):
                    flat_corr.append((feat, ret_col, float(row[ret_col])))
        if flat_corr:
            flat_corr_sorted = sorted(flat_corr, key=lambda x: abs(x[2]), reverse=True)
            print("Top correlations (by |corr|):")
            for feat, ret_col, c in flat_corr_sorted[:10]:
                print(f"  {feat} vs {ret_col}: corr={c:.3f}")
        else:
            print("No valid correlations computed.")
    else:
        print("No correlation table generated.")

    if not bigw_df.empty and "Forward_Return" in bigw_df.columns:
        print(f"\nBig winners count: {len(bigw_df)}")
        print(f"Forward_Return mean:  {bigw_df['Forward_Return'].mean():.2%}")
        print(f"Forward_Return median:{bigw_df['Forward_Return'].median():.2%}")


if __name__ == "__main__":
    main()
